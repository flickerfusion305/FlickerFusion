env: mpe_fluid

render_args:
  srk: [1,2,4]
  sr_color: ["#2CAFAC","#FB5607", "#1982C4"]

twoagent: False
agentnames:
  - agent
n_agents: 7

env_args:
  scenario_id: "simple_adversary.py"
  scenario_config:
    episode_limit: 150
    max_n_agents: 7
    max_n_advs: 7
    max_n_targets: 3
    max_n_decoys: 3
    
    # train(in-domain)
    num_agents: [1, 3]
    num_adversaries: [1, 3]
    num_targets: [1, 2]
    num_decoys: [1, 2]
    intra_trajectory: [1, 1, 1, 1]

    empirical_study: 0 # 0 for same config with train / 1, 2 for OOD 1, 2
    OOD:      
      # OOD 1
      - num_agents: [5]
        num_adversaries: [5]
        num_targets: [1, 2]
        num_decoys: [1, 2]
        intra_trajectory: [2, 0, 1, 1]
      # OOD 2
      - num_agents: [5]
        num_adversaries: [5]
        num_targets: [1, 2]
        num_decoys: [1, 2]
        intra_trajectory: [0, 2, 1, 1]

    delta_entities: # only keys got meaning
      add_agents: [50, 70, 23] 
      add_advs: [50, 90, 129] 
      add_targets: [23, 48, 18]
      add_decoys: [12]
    
    dropout:
      False

# offline dataset
offline_data_folder: "dataset"
offline_data_name: "adversary"
offline_data_name: "replay_memory/123456_QMIX_simple_adversary"
offline_data_quality: "medium"
offline_data_size: 2000
offline_data_shuffle: False
