name: QMIX_UPDET

# # use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0 #epy
epsilon_finish: 0.05 #mrlib
epsilon_anneal_time: 500000 # 500000 for marl #h
epsilon_anneal_start: 0

runner: "parallel"
batch_size_run: 8 # batch_size_run=4, buffer_size = 2500, batch_size=64  for 3s5z_vs_3s6z
training_iters: 8

batch_size: 32 # Number of episodes to train on
buffer_size: 5000 #epy
t_max: 3000000 # Stop running after this many timesteps

# update the target network every {} episodes
target_update_interval: 200 #200 marllib #h

#agent network
agent: "updet" # Options [updet, transformer_aggregation, rnn]
obs_agent_id: False # Include the agent's one_hot id in the observation
obs_last_action: False # Include the agent's last action (one_hot) in the observation

# --- Transformer parameters. Should be set manually. ---
emb: 64 # embedding dimension of transformer
heads: 4 # head number of transformer
depth: 2 # block number of transformer

# use the Q_Learner to train
agent_output_type: "q" #epy
learner: "q_learner" #epy
double_q: True #epy
mixer: "qmix" #epy
mixing_embed_dim: 32 #epy
hypernet_layers: 2 #epy
hypernet_embed: 128 #epy

optimizer: "rmsprop" # "adam" marllib
lr: 0.0003 #h

# use the Q_Learner to train
reward_standardization: True # Reward Standardization

env_args:
  entity_scheme: False
